{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "QS7mq50DVKZt",
        "5yga1ylizK3q"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Preprocessing**"
      ],
      "metadata": {
        "id": "tnttzAlrde3m"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ty-hVIYueo7m"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv('updated_hate_speech2.csv',engine='python')\n",
        "X = data['Content'].values\n",
        "y = data['Label'].values\n",
        "\n",
        "# We did a 80/20 split for training and testing. We later split the training set into training and validation\n",
        "train_X, test_X, train_y, test_y = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "OepVvs93d5-Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Naive Bayes**"
      ],
      "metadata": {
        "id": "QLwFbijVhZ_t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We chose a Multinomial Naive Bayes because it works best with discrete features such as word counts or frequencies"
      ],
      "metadata": {
        "id": "H1wZjoGHfuy2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score"
      ],
      "metadata": {
        "id": "xz3DoG3ZPYe8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use a TF-IDF vectorizer to preprocess the data to create vectors of the word frequencies\n",
        "vectorizer = TfidfVectorizer(stop_words='english')\n",
        "nb_X_train = vectorizer.fit_transform(train_X)\n",
        "nb_X_test = vectorizer.transform(test_X)"
      ],
      "metadata": {
        "id": "_I85bP6BPMLU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "naive = MultinomialNB()\n",
        "naive.fit(nb_X_train, train_y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 75
        },
        "id": "L0sL8DpRP7NF",
        "outputId": "9ca69c5b-41b7-416d-fdea-3f1f20745b27"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MultinomialNB()"
            ],
            "text/html": [
              "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MultinomialNB()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MultinomialNB</label><div class=\"sk-toggleable__content\"><pre>MultinomialNB()</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model\n",
        "y_pred = naive.predict(nb_X_test)\n",
        "y_pred = (y_pred > 0.5).astype('int32')\n",
        "print('Accuracy: %.3f' % accuracy_score(test_y, y_pred))\n",
        "\t\n",
        "print('Precision: %.3f' % precision_score(test_y, y_pred))\n",
        "\t\n",
        "print('Recall: %.3f' % recall_score(test_y, y_pred))\n",
        "\t\n",
        "print('F1: %.3f' % f1_score(test_y, y_pred))"
      ],
      "metadata": {
        "id": "0Idx_Zl1ethI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eddc826e-b172-4fff-d324-29bbdf5842fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.800\n",
            "Precision: 0.771\n",
            "Recall: 0.780\n",
            "F1: 0.776\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **CNN**"
      ],
      "metadata": {
        "id": "lOagRMHpgItY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Conv1D, GlobalMaxPooling1D, Dense, Dropout, Flatten\n",
        "from keras.utils import pad_sequences\n",
        "from keras.layers import Embedding, Conv1D, MaxPooling1D, Dense, Dropout, Flatten\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
        "from keras.callbacks import EarlyStopping"
      ],
      "metadata": {
        "id": "-XLa7ATwgM5g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_X, val_X, train_y, val_y = train_test_split(train_X, train_y, test_size=0.25, random_state=42)"
      ],
      "metadata": {
        "id": "VTw_oRzoSGrS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_embeddings = {}\n",
        "with open('glove.6B.100d.txt', 'r', encoding='utf-8') as f:\n",
        "    for line in f:\n",
        "        values = line.split()\n",
        "        word = values[0]\n",
        "        embedding = np.asarray(values[1:], dtype='float32')\n",
        "        word_embeddings[word] = embedding"
      ],
      "metadata": {
        "id": "HglATWOgRNg7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Could include num_words = 500\n",
        "tokenizer = Tokenizer()\n",
        "#This tokenizes the text and counts the frequency of each token\n",
        "tokenizer.fit_on_texts(train_X)\n",
        "#create a vocabulary of the most frequently occurring words in the training data\n",
        "cnn_X_train = tokenizer.texts_to_sequences(train_X)\n",
        "cnn_X_val = tokenizer.texts_to_sequences(val_X)\n",
        "cnn_X_test = tokenizer.texts_to_sequences(test_X)"
      ],
      "metadata": {
        "id": "gn77FHx8RWl-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We need to pad the sequences here so they have the right shape\n",
        "maxlen = 100\n",
        "cnn_X_train = pad_sequences(cnn_X_train, padding='post', maxlen=maxlen)\n",
        "cnn_X_val = pad_sequences(cnn_X_val, padding='post', maxlen=maxlen)\n",
        "cnn_X_test = pad_sequences(cnn_X_test, padding='post', maxlen=maxlen)"
      ],
      "metadata": {
        "id": "muG0FBB5Setb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Making the matrix for the embedding layer\n",
        "word_index = tokenizer.word_index\n",
        "embedding_dim = 100\n",
        "embedding_matrix = np.zeros((len(word_index) + 1, embedding_dim))\n",
        "\n",
        "for word, i in word_index.items():\n",
        "    embedding_vector = word_embeddings.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        embedding_matrix[i] = embedding_vector\n",
        "    else:\n",
        "        # If word is not in pre-trained embeddings, use random vector\n",
        "        embedding_matrix[i] = np.random.normal(scale=0.6, size=(embedding_dim,))"
      ],
      "metadata": {
        "id": "PGhYacqVSypl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cnn = Sequential()\n",
        "cnn.add(Embedding(input_dim=len(word_index) + 1, output_dim=100, input_length=maxlen, weights=[embedding_matrix], trainable=False))\n",
        "cnn.add(Conv1D(filters=32, kernel_size=3, activation='relu'))\n",
        "cnn.add(MaxPooling1D(pool_size=2))\n",
        "cnn.add(Flatten())\n",
        "cnn.add(Dense(units=250, activation='relu'))\n",
        "cnn.add(Dropout(rate=0.2))\n",
        "cnn.add(Dense(units=1, activation='sigmoid'))"
      ],
      "metadata": {
        "id": "Hrj2u5XlTw8O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cnn.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "cnn.fit(cnn_X_train, train_y, epochs=10, batch_size=64, validation_data=(cnn_X_val, val_y), callbacks=[EarlyStopping(patience=3)])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FYeth8UIUNJX",
        "outputId": "c1cdac44-942b-48fb-d598-1b2f925aa9a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "1681/1681 [==============================] - 53s 31ms/step - loss: 0.4779 - accuracy: 0.7661 - val_loss: 0.4335 - val_accuracy: 0.7952\n",
            "Epoch 2/10\n",
            "1681/1681 [==============================] - 45s 27ms/step - loss: 0.4157 - accuracy: 0.8077 - val_loss: 0.4293 - val_accuracy: 0.7983\n",
            "Epoch 3/10\n",
            "1681/1681 [==============================] - 44s 26ms/step - loss: 0.3841 - accuracy: 0.8252 - val_loss: 0.4152 - val_accuracy: 0.8088\n",
            "Epoch 4/10\n",
            "1681/1681 [==============================] - 45s 27ms/step - loss: 0.3529 - accuracy: 0.8419 - val_loss: 0.4190 - val_accuracy: 0.8081\n",
            "Epoch 5/10\n",
            "1681/1681 [==============================] - 47s 28ms/step - loss: 0.3200 - accuracy: 0.8580 - val_loss: 0.4444 - val_accuracy: 0.8027\n",
            "Epoch 6/10\n",
            "1681/1681 [==============================] - 45s 27ms/step - loss: 0.2901 - accuracy: 0.8734 - val_loss: 0.4674 - val_accuracy: 0.8001\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f449f2f2940>"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = cnn.predict(cnn_X_test)\n",
        "y_pred = (y_pred > 0.5).astype('int32')\n",
        "\n",
        "print('Accuracy: %.3f' % accuracy_score(test_y, y_pred))\n",
        "\t\n",
        "print('Precision: %.3f' % precision_score(test_y, y_pred))\n",
        "\t\n",
        "print('Recall: %.3f' % recall_score(test_y, y_pred))\n",
        "\t\n",
        "print('F1: %.3f' % f1_score(test_y, y_pred))"
      ],
      "metadata": {
        "id": "0HRn70Iyg6jU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0d12b9d9-fbf0-427f-c14d-a3aaf5d9f3b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1121/1121 [==============================] - 5s 5ms/step\n",
            "Accuracy: 0.805\n",
            "Precision: 0.790\n",
            "Recall: 0.762\n",
            "F1: 0.776\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **RNN**"
      ],
      "metadata": {
        "id": "DsqnzfukgvVf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Embedding, LSTM, GRU, Bidirectional\n",
        "from sklearn.metrics import classification_report\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from keras.utils import pad_sequences\n",
        "from keras.callbacks import EarlyStopping"
      ],
      "metadata": {
        "id": "UhG9dkRElYbd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_X, val_X, train_y, val_y = train_test_split(train_X, train_y, test_size=0.25, random_state=42)"
      ],
      "metadata": {
        "id": "ZVEWzq8qSI_D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_embeddings = {}\n",
        "with open('glove.6B.100d.txt', 'r', encoding='utf-8') as f:\n",
        "    for line in f:\n",
        "        values = line.split()\n",
        "        word = values[0]\n",
        "        embedding = np.asarray(values[1:], dtype='float32')\n",
        "        word_embeddings[word] = embedding"
      ],
      "metadata": {
        "id": "95SRRI0pYzrl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Could include num_words = 500\n",
        "tokenizer = Tokenizer()\n",
        "#This tokenizes the text and counts the frequency of each token\n",
        "tokenizer.fit_on_texts(train_X)\n",
        "#create a vocabulary of the most frequently occurring words in the training data\n",
        "rnn_X_train = tokenizer.texts_to_sequences(train_X)\n",
        "rnn_X_val = tokenizer.texts_to_sequences(val_X)\n",
        "rnn_X_test = tokenizer.texts_to_sequences(test_X)"
      ],
      "metadata": {
        "id": "muYJiHvcY0RC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We need to pad the sequences here so they have the right shape\n",
        "maxlen = 100\n",
        "rnn_X_train = pad_sequences(rnn_X_train, padding='post', maxlen=maxlen)\n",
        "rnn_X_val = pad_sequences(rnn_X_val, padding='post', maxlen=maxlen)\n",
        "rnn_X_test = pad_sequences(rnn_X_test, padding='post', maxlen=maxlen)"
      ],
      "metadata": {
        "id": "Te68LBW5Y7KI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Making the matrix for the embedding layer\n",
        "word_index = tokenizer.word_index\n",
        "embedding_dim = 100\n",
        "embedding_matrix = np.zeros((len(word_index) + 1, embedding_dim))\n",
        "\n",
        "for word, i in word_index.items():\n",
        "    embedding_vector = word_embeddings.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        embedding_matrix[i] = embedding_vector\n",
        "    else:\n",
        "        # If word is not in pre-trained embeddings, use random vector\n",
        "        embedding_matrix[i] = np.random.normal(scale=0.6, size=(embedding_dim,))"
      ],
      "metadata": {
        "id": "yieXKjpuYuO2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rnn = Sequential()\n",
        "rnn.add(Embedding(len(word_index) + 1, embedding_dim, input_length=maxlen, \n",
        "                    weights=[embedding_matrix], trainable=False))\n",
        "rnn.add(LSTM(64))\n",
        "rnn.add(Dense(1, activation='sigmoid'))"
      ],
      "metadata": {
        "id": "lc7OlHHTlY08"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rnn.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "rnn.fit(rnn_X_train, train_y, epochs=10, batch_size=64, validation_data=(rnn_X_val, val_y), callbacks=[EarlyStopping(patience=3)])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "njDHlXQwZTF9",
        "outputId": "7a995d33-65cb-458f-f921-12540fb82489"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "1261/1261 [==============================] - 143s 112ms/step - loss: 0.6218 - accuracy: 0.6320 - val_loss: 0.5400 - val_accuracy: 0.7198\n",
            "Epoch 2/10\n",
            "1261/1261 [==============================] - 146s 116ms/step - loss: 0.4892 - accuracy: 0.7591 - val_loss: 0.4529 - val_accuracy: 0.7813\n",
            "Epoch 3/10\n",
            "1261/1261 [==============================] - 143s 114ms/step - loss: 0.4394 - accuracy: 0.7921 - val_loss: 0.4373 - val_accuracy: 0.7942\n",
            "Epoch 4/10\n",
            "1261/1261 [==============================] - 144s 114ms/step - loss: 0.4171 - accuracy: 0.8060 - val_loss: 0.4311 - val_accuracy: 0.7957\n",
            "Epoch 5/10\n",
            "1261/1261 [==============================] - 141s 112ms/step - loss: 0.3992 - accuracy: 0.8164 - val_loss: 0.3974 - val_accuracy: 0.8182\n",
            "Epoch 6/10\n",
            "1261/1261 [==============================] - 146s 116ms/step - loss: 0.3846 - accuracy: 0.8256 - val_loss: 0.3970 - val_accuracy: 0.8220\n",
            "Epoch 7/10\n",
            "1261/1261 [==============================] - 139s 110ms/step - loss: 0.3722 - accuracy: 0.8320 - val_loss: 0.3919 - val_accuracy: 0.8216\n",
            "Epoch 8/10\n",
            "1261/1261 [==============================] - 145s 115ms/step - loss: 0.3589 - accuracy: 0.8400 - val_loss: 0.3910 - val_accuracy: 0.8230\n",
            "Epoch 9/10\n",
            "1261/1261 [==============================] - 144s 115ms/step - loss: 0.3470 - accuracy: 0.8459 - val_loss: 0.3919 - val_accuracy: 0.8237\n",
            "Epoch 10/10\n",
            "1261/1261 [==============================] - 145s 115ms/step - loss: 0.3338 - accuracy: 0.8517 - val_loss: 0.3994 - val_accuracy: 0.8231\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f449f2cd520>"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = rnn.predict(rnn_X_test)\n",
        "y_pred = (y_pred > 0.5).astype('int32')\n",
        "\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
        "print('Accuracy: %.3f' % accuracy_score(test_y, y_pred))\n",
        "\t\n",
        "print('Precision: %.3f' % precision_score(test_y, y_pred))\n",
        "\t\n",
        "print('Recall: %.3f' % recall_score(test_y, y_pred))\n",
        "\t\n",
        "print('F1: %.3f' % f1_score(test_y, y_pred))"
      ],
      "metadata": {
        "id": "svqkoWDVfV7F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "913e19b0-55f4-4530-8e35-c6140057a2c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1121/1121 [==============================] - 22s 19ms/step\n",
            "Accuracy: 0.822\n",
            "Precision: 0.762\n",
            "Recall: 0.868\n",
            "F1: 0.812\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Combined CNN-LSTM**"
      ],
      "metadata": {
        "id": "r4iLqAtehiuD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We wanted to combine the CNN and RNN as we believe it will capture both short-distance and long-distance dependencies"
      ],
      "metadata": {
        "id": "YKtOUpYjhxwl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, Conv1D, MaxPooling1D, Bidirectional\n",
        "from keras.models import Model\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.utils import to_categorical\n",
        "import pandas as pd\n",
        "from keras.utils import pad_sequences\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Flatten"
      ],
      "metadata": {
        "id": "fNj4wDC3hYwE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_X, val_X, train_y, val_y = train_test_split(train_X, train_y, test_size=0.25, random_state=42)"
      ],
      "metadata": {
        "id": "Bljh41HESKXI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_embeddings = {}\n",
        "with open('glove.6B.100d.txt', 'r', encoding='utf-8') as f:\n",
        "    for line in f:\n",
        "        values = line.split()\n",
        "        word = values[0]\n",
        "        embedding = np.asarray(values[1:], dtype='float32')\n",
        "        word_embeddings[word] = embedding"
      ],
      "metadata": {
        "id": "I57EFaWWjas_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Could include num_words = 500\n",
        "tokenizer = Tokenizer()\n",
        "#This tokenizes the text and counts the frequency of each token\n",
        "tokenizer.fit_on_texts(train_X)\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "#create a vocabulary of the most frequently occurring words in the training data\n",
        "combined_X_train = tokenizer.texts_to_sequences(train_X)\n",
        "combined_X_val = tokenizer.texts_to_sequences(val_X)\n",
        "combined_X_test = tokenizer.texts_to_sequences(test_X)"
      ],
      "metadata": {
        "id": "eyU3dtMHjdcy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We need to pad the sequences here so they have the right shape\n",
        "maxlen = 100\n",
        "combined_X_train = pad_sequences(combined_X_train, padding='post', maxlen=maxlen)\n",
        "combined_X_val = pad_sequences(combined_X_val, padding='post', maxlen=maxlen)\n",
        "combined_X_test = pad_sequences(combined_X_test, padding='post', maxlen=maxlen)"
      ],
      "metadata": {
        "id": "wml9jjesjmaw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Making the matrix for the embedding layer\n",
        "word_index = tokenizer.word_index\n",
        "embedding_dim = 100\n",
        "embedding_matrix = np.zeros((len(word_index) + 1, embedding_dim))\n",
        "\n",
        "for word, i in word_index.items():\n",
        "    embedding_vector = word_embeddings.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        embedding_matrix[i] = embedding_vector\n",
        "    else:\n",
        "        # If word is not in pre-trained embeddings, use random vector\n",
        "        embedding_matrix[i] = np.random.normal(scale=0.6, size=(embedding_dim,))"
      ],
      "metadata": {
        "id": "h8CLHW_jju5H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = Input(shape=(maxlen,))\n",
        "embedding_layer = Embedding(vocab_size, embedding_dim, weights=[embedding_matrix], input_length=maxlen, trainable=False)(inputs)\n",
        "conv_layer = Conv1D(filters=64, kernel_size=3, padding='valid', activation='relu')(embedding_layer)\n",
        "pooling_layer = MaxPooling1D(pool_size=2)(conv_layer)\n",
        "\n",
        "lstm_layer = Bidirectional(LSTM(64))(pooling_layer)\n",
        "fc_layer = Dropout(0.5)(lstm_layer)\n",
        "\n",
        "outputs = Dense(1, activation='sigmoid')(fc_layer)\n",
        "CNNLSTM = Model(inputs=inputs, outputs=outputs)"
      ],
      "metadata": {
        "id": "AOfD5xXuiAAQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "CNNLSTM.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "# I used early stopping here to prevent overfitting since this model is prone to overfitting\n",
        "CNNLSTM.fit(combined_X_train, train_y, epochs=10, batch_size=128, validation_data=(combined_X_val, val_y), callbacks=[EarlyStopping(patience=3)])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pc3CBMBZmQia",
        "outputId": "3a1b939b-9eb9-4a59-d04e-f2a8665ada86"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "473/473 [==============================] - 109s 220ms/step - loss: 0.5075 - accuracy: 0.7445 - val_loss: 0.4597 - val_accuracy: 0.7832\n",
            "Epoch 2/10\n",
            "473/473 [==============================] - 102s 217ms/step - loss: 0.4425 - accuracy: 0.7917 - val_loss: 0.4286 - val_accuracy: 0.8023\n",
            "Epoch 3/10\n",
            "473/473 [==============================] - 102s 215ms/step - loss: 0.4137 - accuracy: 0.8090 - val_loss: 0.4179 - val_accuracy: 0.8081\n",
            "Epoch 4/10\n",
            "473/473 [==============================] - 102s 216ms/step - loss: 0.3959 - accuracy: 0.8196 - val_loss: 0.4090 - val_accuracy: 0.8110\n",
            "Epoch 5/10\n",
            "473/473 [==============================] - 102s 216ms/step - loss: 0.3782 - accuracy: 0.8288 - val_loss: 0.4089 - val_accuracy: 0.8110\n",
            "Epoch 6/10\n",
            "473/473 [==============================] - 102s 216ms/step - loss: 0.3618 - accuracy: 0.8386 - val_loss: 0.4168 - val_accuracy: 0.8140\n",
            "Epoch 7/10\n",
            "473/473 [==============================] - 102s 216ms/step - loss: 0.3472 - accuracy: 0.8460 - val_loss: 0.4209 - val_accuracy: 0.8107\n",
            "Epoch 8/10\n",
            "473/473 [==============================] - 102s 216ms/step - loss: 0.3358 - accuracy: 0.8522 - val_loss: 0.4152 - val_accuracy: 0.8112\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f449ed9f850>"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = CNNLSTM.predict(combined_X_test)\n",
        "y_pred = (y_pred > 0.5).astype('int32')\n",
        "\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
        "print('Accuracy: %.3f' % accuracy_score(test_y, y_pred))\n",
        "\t\n",
        "print('Precision: %.3f' % precision_score(test_y, y_pred))\n",
        "\t\n",
        "print('Recall: %.3f' % recall_score(test_y, y_pred))\n",
        "\t\n",
        "print('F1: %.3f' % f1_score(test_y, y_pred))"
      ],
      "metadata": {
        "id": "-H79F5pYicoJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "04c97da4-2502-4295-b292-f36913a9c799"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1121/1121 [==============================] - 23s 20ms/step\n",
            "Accuracy: 0.815\n",
            "Precision: 0.783\n",
            "Recall: 0.804\n",
            "F1: 0.794\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Outside Testing**"
      ],
      "metadata": {
        "id": "gbWOr_ZqhH2j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## HSD Dataset"
      ],
      "metadata": {
        "id": "QS7mq50DVKZt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random \n",
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ],
      "metadata": {
        "id": "o-vrk7ML9sKo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "outside_data = pd.read_csv('merged_hate.csv',engine='python')\n",
        "test_X = outside_data['contents'].values\n",
        "new_test_y = outside_data['label'].values"
      ],
      "metadata": {
        "id": "ZM5ouH8rVJtc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Naive Bayes**"
      ],
      "metadata": {
        "id": "3x7JPM4UZ6XT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit vectorizer on testing data\n",
        "nb_new_test_X = vectorizer.transform(test_X)"
      ],
      "metadata": {
        "id": "y9ykaGVrVOlD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model\n",
        "y_pred = naive.predict(nb_new_test_X)\n",
        "y_pred = (y_pred > 0.5).astype('int32')\n",
        "print('Accuracy: %.3f' % accuracy_score(new_test_y, y_pred))\n",
        "\t\n",
        "print('Precision: %.3f' % precision_score(new_test_y, y_pred))\n",
        "\t\n",
        "print('Recall: %.3f' % recall_score(new_test_y, y_pred))\n",
        "\t\n",
        "print('F1: %.3f' % f1_score(new_test_y, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S6UQ1im9VS-Z",
        "outputId": "07fdecfe-d3c0-44ba-a514-02b9fb54ad0c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.763\n",
            "Precision: 0.752\n",
            "Recall: 0.783\n",
            "F1: 0.767\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**CNN**"
      ],
      "metadata": {
        "id": "oDsYs1FfaA2o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cnn_X_new_test = tokenizer.texts_to_sequences(test_X)\n",
        "cnn_X_new_test = pad_sequences(cnn_X_new_test, padding='post', maxlen=maxlen)\n",
        "\n",
        "# make predictions on the test data\n",
        "y_pred = cnn.predict(cnn_X_new_test)\n",
        "y_pred = (y_pred > 0.5).astype('int32')\n",
        "\n",
        "# evaluate the model's performance\n",
        "print('Accuracy: %.3f' % accuracy_score(new_test_y, y_pred))\n",
        "\n",
        "print('Precision: %.3f' % precision_score(new_test_y, y_pred))\n",
        "\n",
        "print('Recall: %.3f' % recall_score(new_test_y, y_pred))\n",
        "\n",
        "print('F1: %.3f' % f1_score(new_test_y, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ila-G6aktNwn",
        "outputId": "023297a9-1a3b-4f6e-cbaf-78c96c64a853"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "75/75 [==============================] - 0s 5ms/step\n",
            "Accuracy: 0.531\n",
            "Precision: 0.554\n",
            "Recall: 0.311\n",
            "F1: 0.399\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**RNN**"
      ],
      "metadata": {
        "id": "70-3XqEJt9Dt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rnn_X_new_test = tokenizer.texts_to_sequences(test_X)\n",
        "rnn_X_new_test = pad_sequences(rnn_X_new_test, padding='post', maxlen=maxlen)\n",
        "\n",
        "y_pred = rnn.predict(rnn_X_new_test)\n",
        "y_pred = (y_pred > 0.5).astype('int32')\n",
        "\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
        "print('Accuracy: %.3f' % accuracy_score(new_test_y, y_pred))\n",
        "\t\n",
        "print('Precision: %.3f' % precision_score(new_test_y, y_pred))\n",
        "\t\n",
        "print('Recall: %.3f' % recall_score(new_test_y, y_pred))\n",
        "\t\n",
        "print('F1: %.3f' % f1_score(new_test_y, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FyQYf1bJuA5y",
        "outputId": "2e8d4206-46bb-45b1-c4b7-e671d302ed7d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "75/75 [==============================] - 1s 19ms/step\n",
            "Accuracy: 0.535\n",
            "Precision: 0.545\n",
            "Recall: 0.417\n",
            "F1: 0.473\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Combined CNN-LSTM**"
      ],
      "metadata": {
        "id": "AnIaFypxu2r7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "combined_X_test = tokenizer.texts_to_sequences(test_X)\n",
        "combined_X_test = pad_sequences(combined_X_test, padding='post', maxlen=maxlen)\n",
        "\n",
        "y_pred = CNNLSTM.predict(combined_X_test)\n",
        "y_pred = (y_pred > 0.5).astype('int32')\n",
        "\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
        "print('Accuracy: %.3f' % accuracy_score(new_test_y, y_pred))\n",
        "\t\n",
        "print('Precision: %.3f' % precision_score(new_test_y, y_pred))\n",
        "\t\n",
        "print('Recall: %.3f' % recall_score(new_test_y, y_pred))\n",
        "\t\n",
        "print('F1: %.3f' % f1_score(new_test_y, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZoayfYPtuzWR",
        "outputId": "71aacdc1-9146-4c80-d513-8d051e91df55"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "75/75 [==============================] - 2s 21ms/step\n",
            "Accuracy: 0.745\n",
            "Precision: 0.762\n",
            "Recall: 0.712\n",
            "F1: 0.736\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Davidson Hate Speech Dataset"
      ],
      "metadata": {
        "id": "5yga1ylizK3q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "davidson_data = pd.read_csv('davidson_data.csv',engine='python')\n",
        "test_X = davidson_data['tweet'].values\n",
        "new_test_y = davidson_data['class'].values"
      ],
      "metadata": {
        "id": "HxmfVETRzPYv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Naive Bayes**"
      ],
      "metadata": {
        "id": "0NkP7mYszaVO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit vectorizer on testing data\n",
        "nb_new_test_X = vectorizer.transform(test_X)\n",
        "\n",
        "# Evaluate the model\n",
        "y_pred = naive.predict(nb_new_test_X)\n",
        "y_pred = (y_pred > 0.5).astype('int32')\n",
        "print('Accuracy: %.3f' % accuracy_score(new_test_y, y_pred))\n",
        "\t\n",
        "print('Precision: %.3f' % precision_score(new_test_y, y_pred))\n",
        "\t\n",
        "print('Recall: %.3f' % recall_score(new_test_y, y_pred))\n",
        "\t\n",
        "print('F1: %.3f' % f1_score(new_test_y, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7scND61wzf9R",
        "outputId": "51e52a2f-42ba-4aaa-b9e4-8e8eaf723cca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.801\n",
            "Precision: 0.753\n",
            "Recall: 0.897\n",
            "F1: 0.819\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**CNN**"
      ],
      "metadata": {
        "id": "lEi_BzB0zxGb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cnn_X_new_test = tokenizer.texts_to_sequences(test_X)\n",
        "cnn_X_new_test = pad_sequences(cnn_X_new_test, padding='post', maxlen=maxlen)\n",
        "\n",
        "# make predictions on the test data\n",
        "y_pred = cnn.predict(cnn_X_new_test)\n",
        "y_pred = (y_pred > 0.5).astype('int32')\n",
        "\n",
        "# evaluate the model's performance\n",
        "print('Accuracy: %.3f' % accuracy_score(new_test_y, y_pred))\n",
        "\n",
        "print('Precision: %.3f' % precision_score(new_test_y, y_pred))\n",
        "\n",
        "print('Recall: %.3f' % recall_score(new_test_y, y_pred))\n",
        "\n",
        "print('F1: %.3f' % f1_score(new_test_y, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uY9frV_MzwbQ",
        "outputId": "f51bb675-c902-45f6-b1a8-2407bc588c39"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "90/90 [==============================] - 0s 5ms/step\n",
            "Accuracy: 0.513\n",
            "Precision: 0.522\n",
            "Recall: 0.319\n",
            "F1: 0.396\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**RNN**"
      ],
      "metadata": {
        "id": "Av-6V361z9WC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rnn_X_new_test = tokenizer.texts_to_sequences(test_X)\n",
        "rnn_X_new_test = pad_sequences(rnn_X_new_test, padding='post', maxlen=maxlen)\n",
        "\n",
        "y_pred = rnn.predict(rnn_X_new_test)\n",
        "y_pred = (y_pred > 0.5).astype('int32')\n",
        "\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
        "print('Accuracy: %.3f' % accuracy_score(new_test_y, y_pred))\n",
        "\t\n",
        "print('Precision: %.3f' % precision_score(new_test_y, y_pred))\n",
        "\t\n",
        "print('Recall: %.3f' % recall_score(new_test_y, y_pred))\n",
        "\t\n",
        "print('F1: %.3f' % f1_score(new_test_y, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iClIIJlsz_ic",
        "outputId": "f0d4c241-7074-4518-d74f-95afedc8adaa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "90/90 [==============================] - 2s 27ms/step\n",
            "Accuracy: 0.477\n",
            "Precision: 0.481\n",
            "Recall: 0.576\n",
            "F1: 0.524\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Combined CNN-LSTM**"
      ],
      "metadata": {
        "id": "KD1RSiKl0GHx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "combined_X_test = tokenizer.texts_to_sequences(test_X)\n",
        "combined_X_test = pad_sequences(combined_X_test, padding='post', maxlen=maxlen)\n",
        "\n",
        "y_pred = CNNLSTM.predict(combined_X_test)\n",
        "y_pred = (y_pred > 0.5).astype('int32')\n",
        "\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
        "print('Accuracy: %.3f' % accuracy_score(new_test_y, y_pred))\n",
        "\t\n",
        "print('Precision: %.3f' % precision_score(new_test_y, y_pred))\n",
        "\t\n",
        "print('Recall: %.3f' % recall_score(new_test_y, y_pred))\n",
        "\t\n",
        "print('F1: %.3f' % f1_score(new_test_y, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "38e_An1n0LYP",
        "outputId": "e1ccdb5a-e239-454b-91d9-bd5d66ed68e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "90/90 [==============================] - 2s 19ms/step\n",
            "Accuracy: 0.731\n",
            "Precision: 0.702\n",
            "Recall: 0.806\n",
            "F1: 0.750\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, we decided to bring in another database unrelated to our training set to see how our models each do"
      ],
      "metadata": {
        "id": "jZhwrDychKRk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "HSD: The Hate Speech Dataset: https://github.com/aitor-garcia-p/hate-speech-dataset\n",
        "\n",
        "Stormfront Corpus: https://paperswithcode.com/dataset/hate-speech\n",
        "\n",
        "Wikipedia Talk Pages: https://figshare.com/articles/dataset/Wikipedia_Talk_Labels_Toxicity/4563973\n",
        "\n",
        "\n",
        "The Davidson Hate Speech Dataset: https://github.com/t-davidson/hate-speech-and-offensive-language"
      ],
      "metadata": {
        "id": "xD18bRlz9ry2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I would do the HSD and the Stormfront corpus, load them in and test the 4 trained models on them"
      ],
      "metadata": {
        "id": "Pfca7VyoeHzP"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "H8_08VAlhRNg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}